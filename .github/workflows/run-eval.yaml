name: Run API Evaluation

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: '29 2 * * 1'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref || github.run_id }}
  cancel-in-progress: true

jobs:
  run-api-eval:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: |
            pyproject.toml

      - name: Install dependencies
        run: |
          bin/api/prepare.sh

      - name: Get Available Model
        id: get_model
        env:
          MODEL_URL: ${{ vars.MODEL_URL }}
          MODEL_API_KEY: ${{ secrets.MODEL_API_KEY }}
        run: |
          echo "Fetching available models from $MODEL_URL/models"
          RESPONSE=$(curl -s "$MODEL_URL/models" -H "Authorization: Bearer $MODEL_API_KEY")
          echo "API Response: $RESPONSE"
          
          DATA_LENGTH=$(echo "$RESPONSE" | jq -r '.data | length')
          if [ "$DATA_LENGTH" = "null" ] || [ "$DATA_LENGTH" = "0" ]; then
            echo "Error: No models available in the data array"
            exit 1
          fi
          
          SELECTED_MODEL=$(echo "$RESPONSE" | jq -r '.data[0].id')
          
          if [ "$SELECTED_MODEL" = "null" ] || [ -z "$SELECTED_MODEL" ]; then
            echo "Error: Could not extract model ID from API response"
            exit 1
          fi
          
          echo "Selected model: $SELECTED_MODEL"
          echo "model_name=$SELECTED_MODEL" >> $GITHUB_OUTPUT

      - name: Run API Evaluation Script
        env:
          MODEL_NAME: ${{ steps.get_model.outputs.model_name }}
          MODEL_URL: ${{ vars.MODEL_URL }}
          MODEL_API_KEY: ${{ secrets.MODEL_API_KEY }}
        run: |
          bin/api/run_api_eval.sh \
            --model-name "$MODEL_NAME" \
            --model-url "$MODEL_URL" \
            --model-api-key "$MODEL_API_KEY" \
            --num-choices 1 \
            --question-count 1

      - name: Process Model Answers
        run: |
          OUTPUT_FILE=$(ls -t llm_judge/data/japanese_mt_bench/model_answer/*.jsonl | head -n 1)
          if [ -f "$OUTPUT_FILE" ]; then
            echo "Output JSON file: $OUTPUT_FILE"
            # Print to console with jq
            cat "$OUTPUT_FILE" | jq

            # Write to workflow summary
            echo "# Model Answers" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Output file: \`$OUTPUT_FILE\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat "$OUTPUT_FILE" | jq . >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "No output file found in the directory."
            exit 1
          fi

      - name: Run OpenAI Judge
        env:
          MODEL_NAME: ${{ steps.get_model.outputs.model_name }}
          MODEL_URL: ${{ vars.MODEL_URL }}
          MODEL_API_KEY: ${{ secrets.MODEL_API_KEY }}
        run: |
          # let the model judge itself against the GPT-4 answers
          bin/api/run_openai_judge.sh \
            --model-name "$MODEL_NAME" \
            --judge-model-name "lfm-7b" \
            --judge-model-url "$MODEL_URL" \
            --judge-model-api-key "$MODEL_API_KEY" \
            --parallel 3

      - name: Process Judge Results
        env:
          CI: true
        run: |
          OUTPUT_FILE=$(ls -t llm_judge/data/japanese_mt_bench/model_judgment/*.jsonl | head -n 1)
          if [ -f "$OUTPUT_FILE" ]; then
            echo "Output JSON file: $OUTPUT_FILE"
            # Print to console with jq
            cat "$OUTPUT_FILE" | jq

            # Write to workflow summary
            echo "# Judge Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Output file: \`$OUTPUT_FILE\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat "$OUTPUT_FILE" | jq . >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "No output file found in the directory."
            exit 1
          fi

          ls llm_judge/data/japanese_mt_bench/*.json
          cat llm_judge/data/japanese_mt_bench/*.json

          python3 << 'EOF'
          import json
          import os
          import glob

          def create_model_table(data):
            # Get the model name (first key in the object)
            model_name = next(iter(data))
            model_data = data[model_name]

            # Get all category names (excluding 'result' field)
            categories = [key for key in model_data.keys() if key != 'result']

            # Generate markdown table
            markdown = f'## `{model_name}`\n\n'

            # Create table header
            markdown += '| ' + ' | '.join(categories) + ' |\n'
            markdown += '| ' + ' | '.join(['---'] * len(categories)) + ' |\n'

            # Create table row with average scores
            scores = []
            for category in categories:
              avg_data = model_data[category].get('average', {})
              if avg_data and 'score' in avg_data:
                scores.append(f"{avg_data['score']:.2f}")
              else:
                scores.append('')
            markdown += '| ' + ' | '.join(scores) + ' |\n\n'

            return markdown

          # Start the markdown content
          markdown_content = '# Scores\n\n'

          # Process all JSON files
          json_files = glob.glob('llm_judge/data/japanese_mt_bench/*.json')

          # Sort files to ensure consistent order
          json_files.sort()

          for json_file in json_files:
            try:
              with open(json_file, 'r') as f:
                data = json.load(f)
                markdown_content += create_model_table(data)
            except Exception as e:
              print(f"Error processing {json_file}: {str(e)}")

          # Write to GITHUB_STEP_SUMMARY
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
            f.write(markdown_content)
          EOF
